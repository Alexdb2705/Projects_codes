{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import  matplotlib.pyplot   as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import math\n",
    "from torchsummary import summary\n",
    "\n",
    "torch.manual_seed(101)\n",
    "torch.cuda.manual_seed(101)\n",
    "\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, img1_dir, img2_dir, img3_dir, coords_file, transform=None):\n",
    "        # Directorios de imágenes y archivo CSV de coordenadas\n",
    "        self.img1_dir = img1_dir\n",
    "        self.img2_dir = img2_dir\n",
    "        self.img3_dir = img3_dir\n",
    "        self.coords = np.load(coords_file)[:, 1:]\n",
    "        # self.coords = self.coords.reshape(self.coords.shape[0],int(self.coords.shape[1]/3),3)\n",
    "        # for i in range(self.coords.shape[0]):\n",
    "        #     # Ordenar por las columnas 2 (z), 1 (y) y 0 (x)\n",
    "        #     self.coords[i] = self.coords[i][np.lexsort((self.coords[i][:, 2], self.coords[i][:, 1], self.coords[i][:, 0]))]\n",
    "        # self.coords = self.coords.reshape(self.coords.shape[0],self.coords.shape[1]*self.coords.shape[2])\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.coords)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        # Cargar las tres imágenes\n",
    "        img1_path = os.path.join(self.img1_dir, f\"isar_{idx+1}_1.png\")\n",
    "        img2_path = os.path.join(self.img2_dir, f\"isar_{idx+1}_2.png\")\n",
    "        img3_path = os.path.join(self.img3_dir, f\"isar_{idx+1}_3.png\")\n",
    "        img1 = Image.open(img1_path).convert(\"L\")\n",
    "        img2 = Image.open(img2_path).convert(\"L\")\n",
    "        img3 = Image.open(img3_path).convert(\"L\")\n",
    "\n",
    "        # Aplicar transformaciones si las hay\n",
    "        if self.transform:\n",
    "            img1 = self.transform(img1)\n",
    "            img2 = self.transform(img2)\n",
    "            img3 = self.transform(img3)\n",
    "\n",
    "        # Obtener el vector de coordenadas de salida\n",
    "        coords = torch.tensor(self.coords[idx,:], dtype=torch.float32)\n",
    "\n",
    "        return img1.to(device), img2.to(device), img3.to(device), coords.to(device)\n",
    "\n",
    "class CNNModule(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNModule, self).__init__()\n",
    "        # Capas convolucionales\n",
    "        self.conv1 = nn.LazyConv2d(96, kernel_size=22, stride=2, padding=1)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        self.conv2 = nn.LazyConv2d(256, kernel_size=5, padding=2)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        self.conv3 = nn.LazyConv2d(384, kernel_size=3, padding=1)\n",
    "        self.conv4 = nn.LazyConv2d(384, kernel_size=3, padding=1)\n",
    "        self.conv5 = nn.LazyConv2d(256, kernel_size=3, padding=1)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        self.ReLU = nn.ReLU()\n",
    "        self.Flatten = nn.Flatten()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.ReLU(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.ReLU(x)\n",
    "        x = self.pool2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.ReLU(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.ReLU(x)\n",
    "        x = self.conv5(x)\n",
    "        x = self.ReLU(x)\n",
    "        x = self.pool3(x)\n",
    "        x = self.Flatten(x)\n",
    "        return x\n",
    "    \n",
    "class CoordinatePredictor(nn.Module):\n",
    "    def __init__(self,coords_width):\n",
    "        super(CoordinatePredictor, self).__init__()\n",
    "        # CNN compartida para las tres imágenes\n",
    "        self.cnn = CNNModule()\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.LazyLinear(4096)  # Ajustar el tamaño si las imágenes son diferentes\n",
    "        self.fc2 = nn.LazyLinear(4096)\n",
    "        #self.Dropout = nn.Dropout(p=0.5)\n",
    "        self.fc3 = nn.LazyLinear(coords_width)  # 3*N salidas, una por coordenada (x, y, z) de cada uno de los N vertices\n",
    "\n",
    "    def forward(self, image1, image2, image3):\n",
    "        # Procesar cada imagen a través de la CNN\n",
    "        image1_features = self.cnn(image1)\n",
    "        image2_features = self.cnn(image2)\n",
    "        image3_features = self.cnn(image3)\n",
    "\n",
    "        # Concatenar las características de las tres imágenes\n",
    "        concat_images = torch.cat((image1_features, image2_features, image3_features), dim=1)\n",
    "\n",
    "        # Pasar por las capas densas\n",
    "        x = F.relu(self.fc1(concat_images))\n",
    "        #x = self.Dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        #x = self.Dropout(x)\n",
    "        output_coords = self.fc3(x)\n",
    "\n",
    "        return output_coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 96, 55, 55]          46,560\n",
      "              ReLU-2           [-1, 96, 55, 55]               0\n",
      "         MaxPool2d-3           [-1, 96, 27, 27]               0\n",
      "            Conv2d-4          [-1, 256, 27, 27]         614,656\n",
      "              ReLU-5          [-1, 256, 27, 27]               0\n",
      "         MaxPool2d-6          [-1, 256, 13, 13]               0\n",
      "            Conv2d-7          [-1, 384, 13, 13]         885,120\n",
      "              ReLU-8          [-1, 384, 13, 13]               0\n",
      "            Conv2d-9          [-1, 384, 13, 13]       1,327,488\n",
      "             ReLU-10          [-1, 384, 13, 13]               0\n",
      "           Conv2d-11          [-1, 256, 13, 13]         884,992\n",
      "             ReLU-12          [-1, 256, 13, 13]               0\n",
      "        MaxPool2d-13            [-1, 256, 6, 6]               0\n",
      "          Flatten-14                 [-1, 9216]               0\n",
      "        CNNModule-15                 [-1, 9216]               0\n",
      "           Conv2d-16           [-1, 96, 55, 55]          46,560\n",
      "             ReLU-17           [-1, 96, 55, 55]               0\n",
      "        MaxPool2d-18           [-1, 96, 27, 27]               0\n",
      "           Conv2d-19          [-1, 256, 27, 27]         614,656\n",
      "             ReLU-20          [-1, 256, 27, 27]               0\n",
      "        MaxPool2d-21          [-1, 256, 13, 13]               0\n",
      "           Conv2d-22          [-1, 384, 13, 13]         885,120\n",
      "             ReLU-23          [-1, 384, 13, 13]               0\n",
      "           Conv2d-24          [-1, 384, 13, 13]       1,327,488\n",
      "             ReLU-25          [-1, 384, 13, 13]               0\n",
      "           Conv2d-26          [-1, 256, 13, 13]         884,992\n",
      "             ReLU-27          [-1, 256, 13, 13]               0\n",
      "        MaxPool2d-28            [-1, 256, 6, 6]               0\n",
      "          Flatten-29                 [-1, 9216]               0\n",
      "        CNNModule-30                 [-1, 9216]               0\n",
      "           Conv2d-31           [-1, 96, 55, 55]          46,560\n",
      "             ReLU-32           [-1, 96, 55, 55]               0\n",
      "        MaxPool2d-33           [-1, 96, 27, 27]               0\n",
      "           Conv2d-34          [-1, 256, 27, 27]         614,656\n",
      "             ReLU-35          [-1, 256, 27, 27]               0\n",
      "        MaxPool2d-36          [-1, 256, 13, 13]               0\n",
      "           Conv2d-37          [-1, 384, 13, 13]         885,120\n",
      "             ReLU-38          [-1, 384, 13, 13]               0\n",
      "           Conv2d-39          [-1, 384, 13, 13]       1,327,488\n",
      "             ReLU-40          [-1, 384, 13, 13]               0\n",
      "           Conv2d-41          [-1, 256, 13, 13]         884,992\n",
      "             ReLU-42          [-1, 256, 13, 13]               0\n",
      "        MaxPool2d-43            [-1, 256, 6, 6]               0\n",
      "          Flatten-44                 [-1, 9216]               0\n",
      "        CNNModule-45                 [-1, 9216]               0\n",
      "           Linear-46                 [-1, 4096]     113,250,304\n",
      "           Linear-47                 [-1, 4096]      16,781,312\n",
      "           Linear-48                   [-1, 24]          98,328\n",
      "================================================================\n",
      "Total params: 141,406,392\n",
      "Trainable params: 141,406,392\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 16777216.00\n",
      "Forward/backward pass size (MB): 33.05\n",
      "Params size (MB): 539.42\n",
      "Estimated Total Size (MB): 16777788.47\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Definir las transformaciones (por ejemplo, redimensionar las imágenes y normalizarlas)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),                                        # Cambia esto según el tamaño de tus imágenes\n",
    "    transforms.ToTensor(),                                              # Convertir las imágenes a tensores\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])     # Normalización\n",
    "])\n",
    "\n",
    "# Crear el dataset personalizado\n",
    "dataset = CustomDataset(\n",
    "    img1_dir=\"\\\\Img\\\\ISAR\", \n",
    "    img2_dir=\"\\\\Img\\\\ISAR\", \n",
    "    img3_dir=\"\\\\Img\\\\ISAR\",\n",
    "    coords_file=\"\\\\Img\\\\coords.npy\",\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "# Dividir el dataset en entrenamiento y validación\n",
    "train_size = int(0.7 * len(dataset))  # 70% para entrenamiento\n",
    "val_size = int(0.15 * len(dataset))  # 15% para validación\n",
    "test_size = len(dataset) - train_size - val_size  # 15% para test\n",
    "\n",
    "generator = torch.Generator().manual_seed(101)\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size], generator=generator)\n",
    "\n",
    "# Crear DataLoaders para iterar sobre los datasets\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=len(val_dataset), shuffle=False)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=len(test_dataset), shuffle=False)\n",
    "\n",
    "# Inicializar modelo\n",
    "model = CoordinatePredictor(dataset.coords.shape[1])\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar un modelo\n",
    "model.load_state_dict(torch.load('\\\\Cube_ISAR_1000samples_100ep_32bs_noroot.pth',map_location=torch.device('cpu')))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "# Importante incluir esta linea al comienzo de la celda donde se quiere visualizar el resultado\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "random_ch = np.random.randint(0, len(test_loader))\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, (img1, img2, img3, target_coords) in enumerate(test_loader):\n",
    "        if i == random_ch:\n",
    "            sample_im1 = img1[0,0,:].reshape(1,1,img1.shape[2],img1.shape[3]).to(device)\n",
    "            sample_im2 = img2[0,0,:].reshape(1,1,img2.shape[2],img2.shape[3]).to(device)\n",
    "            sample_im3 = img3[0,0,:].reshape(1,1,img3.shape[2],img3.shape[3]).to(device)\n",
    "            sample_target = target_coords[0,:].reshape(1,-1)\n",
    "            break\n",
    "\n",
    "def plot_sample(output, target):\n",
    "    fig = plt.figure()\n",
    "    ax = Axes3D(fig, auto_add_to_figure=False)\n",
    "    ax.axes.set_xlim3d(left=-2, right=2) \n",
    "    ax.axes.set_ylim3d(bottom=-2, top=2)\n",
    "    ax.axes.set_zlim3d(bottom=-2, top=2) \n",
    "    fig.add_axes(ax)\n",
    "    ax.set_axis_off()\n",
    "\n",
    "    ax.scatter(output[:,0], output[:,1], output[:,2], c='red')\n",
    "    ax.scatter(target[:,0], target[:,1], target[:,2], c='blue')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "output = model(sample_im1, sample_im2, sample_im3)[0].reshape((-1,3)).detach().cpu().numpy()\n",
    "target = sample_target.reshape((-1,3)).detach().cpu().numpy()\n",
    "plot_sample(output,target)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
